# 基于qwen2.5-14B/7B/3B-Insrtruct思维链微调

## 数据

{
	"instruction"："你是一个专业的CNC机械零件加工领域专家。请根据提供的轧辊参数思考分析，并一步步推理，给出对应的加工工序，以json格式输出。",
	"input":"",
	"output":思维链+"最终工序如下："+真实工序
}
## 参数模板

### model
model_name_or_path: /data/models/Qwen2.5-7B-Instruct
new_special_tokens: <think>,</think>
trust_remote_code: true
# resume_from_checkpoint: saves/Qwen2.5-7B-Instruct/lora/sft1/checkpoint-200

### lora
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all

### dataset
dataset: gongxu_alpaca
template: qwen
cutoff_len: 8000
# max_samples: 3000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/Qwen2.5-7B-Instruct/lora/sft1
logging_steps: 10
save_steps: 100
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
# gradient_accumulation_steps: 2
learning_rate: 1.0e-5
num_train_epochs: 10
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: True
ddp_timeout: 180000000

# eval
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 100
deepspeed: /root/LLaMA-	Factory/examples/deepspeed/ds_z3_offload_config.json

其中cutoff_len要尽可能的大，正常应该是8192，满足数据集output的长度
deepspeed开启模型并行加速，此处选择使用z3 offload，将划分优化器参数、梯度与模型参数到cpu,减小npu压力。
## 训练

### 环境变量设置

1.设置使用显卡编号
	export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
2.开启虚拟内存
	export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
	使能内存池扩展段功能，即虚拟内存特性。此设置将指示缓存分配器创建特定的内存块分配，这些内存块后续可以扩展，以便能更好地处理内存使用中频繁变更使用内存大小的情况,从而达到减少内存碎片，降低内存峰值的目的
### 启动lora微调训练

llamafactory-cli train examples/train_lora/qwen_lora_sft.yaml 2>&1 | tee -a log/output_7.log
	2>&1 | tee -a log/output_7.log 保存打印日志
## 问题

### npu:out of memery

显存不足：
解决：
	开启deepspeed z3 offload 开启模型并行，将梯度，模型，优化器部分参数加载到内存，降低显存占用，但仍然会报出该问题。
### 开启模型并行后，仍然提示NPU out of memory. Tried to allocate 10.00 GiB (NPU 0; 32.00 GiB total capacity; 20.40 GiB already allocated; 20.40 GiB current active; 、9.99 GiB free; 20.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.

尝试设置环境变量
export  PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:32,
garbage_collection_threshold:0.6，以解决显存不足问题，减少内存碎片
### 设置后，开始训练时，出现长时间进程不响应

尝试不设置该环境变量参数，替换为环境变量 export  PYTORCH_NPU_ALLOC_CONF=expandable_segments:True，开启虚拟内存
解决，长时间不响应问题
### 训练开启，经过几个step训练，报出错误Kernel task happen error.Failed to allocate memory.Available memory is insufficient.

显示原因是显存不足，降低显存占用。
经过多次降低cutoff_len仍然会出现，只有降低足够小，才能够继续训练。
解决：
	降低cutoff_len
但降低cutoff_len不满足本次微调训练的要求
### 对qwen2.5-7B-instruct模型，尝试设置cutoff_len=7000进行训练，能够完整训练

但训练效果很差。
测试效果不能输出正常语句，其训练loss值出现异常波动

### 设置PYTORCH_NPU_ALLOC_CONF=expandable_segments:True npu仍然报错oom

### ERR00100 PTA call acl api failed

在使用 Ascend NPU 进行推理或训练时，调用 ACL API 失败所导致的。这可能是由于内存异步复制失败、设备驱动不匹配或其他内部错误引起的。
发现：
（1）使用deepspeed 0.16.4版本，开启梯度积累经常出现该错误，使用deepspeed 0.15.4偶尔出现，因此默认禁止开启梯度积累
（2）在其他情况下，也会经常出现该错误 
解决：重启服务器
### npu RuntimeError: ACL stream synchronize failed, error code:507015

### RuntimeError: Initialize:torch_npu/csrc/core/npu/sys_ctrl/npu_sys_ctrl.cpp:217 NPU error, error code is 500001.

### 参考训练配置https://support.huaweicloud.com/bestpractice-modelarts/modelarts_llm_train_90831.html

[Qwen2.5-3B-instruct微调测试](https://uig9b1u98i3.feishu.cn/wiki/OFmtwEGwgiPdz9kC63JcmGranSf)
